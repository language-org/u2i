{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTENT CHARACTERIZATION\n",
    "\n",
    "TO DO :  \n",
    "    * track pipeline w/ airflow  \n",
    "* **workload**: \n",
    "    * 4 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# preprocessing\n",
    "import spacy\n",
    "from spacy import tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# exploration\n",
    "import re\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "pd.set_option(\"display.max_colwidth\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_path = \"/Users/steeve_laquitaine/desktop/CodeHub/intent/intent/\"\n",
    "train_data_path = proj_path + \"data/01_raw/banking77/train.csv\"\n",
    "test_data_path = proj_path + \"data/01_raw/banking77/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('{} unique labels'.format(train_data.category.nunique()))\n",
    "print(train_data.category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "train_data.category.value_counts().plot(\n",
    "    kind='bar', \n",
    "    # y=train_data.category.value_counts().index, \n",
    "    # x=train_data.category.value_counts().values, \n",
    "    figsize=(15,2)\n",
    "    );\n",
    "print('Took {} secs'.format(np.round(time.time()-tic,2)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal plot\n",
    "tic = time.time()\n",
    "train_data.category.value_counts().plot.barh(\n",
    "    figsize=(15,2)\n",
    "    );\n",
    "print('Took {} secs'.format(np.round(time.time()-tic,2)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list labels sorted by occurrence frequency\n",
    "intents_by_popularity = train_data.category.value_counts().index.tolist()\n",
    "intents_by_popularity[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top intent requests\n",
    "top_label_data = train_data.text.loc[train_data.category==intents_by_popularity[0]]\n",
    "top_label_data.tolist()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data Series to one data String\n",
    "data_string = \" \".join(top_label_data.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "def tokenize_txt(data_string, tokenizer, English):\n",
    "    tokenize = tokenizer.Tokenizer(English().vocab)\n",
    "    tokens = tokenize(data_string)\n",
    "    print(\"{} tokens\".format(len(tokens)))\n",
    "    return tokens\n",
    "tokens = tokenize_txt(data_string, tokenizer, English)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe requests\n",
    "\n",
    "* Clause features:\n",
    "\n",
    "  1. **Type**  \n",
    "    \n",
    "        * interrogative (1)\n",
    "            * closed\n",
    "            * open\n",
    "        * declarative\n",
    "        * imperative (2)\n",
    "            * wishes  \n",
    "            * orders  \n",
    "        * exclamative\n",
    "        \n",
    "  2. **Length**\n",
    "\n",
    "  3. **Structural complexity (3)**  \n",
    "        * simple  \n",
    "        * compound    \n",
    "        * complex  \n",
    "        * compound-complex  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_intent = intents_by_popularity[0]\n",
    "top_intent_text = train_data.text[train_data.category.eq(top_intent)].tolist()\n",
    "top_intent_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either ? or ! or .\n",
    "SENT_TYPE_PATTN = re.compile(r\"[\\?\\!\\.]\")\n",
    "\n",
    "def classify_mood(sentences):\n",
    "    \"\"\"\n",
    "    Classify sentence type\n",
    "    \"\"\"\n",
    "    sent_type = []\n",
    "    for sent in sentences:\n",
    "        out = SENT_TYPE_PATTN.findall(sent)\n",
    "        sent_type.append(['ask' if ix=='?' else 'wish-or-excl' if ix=='!' else 'state' for ix in out])        \n",
    "    return sent_type\n",
    "\n",
    "\n",
    "def detect_sentence_type(df, sent_type:str):\n",
    "    \"\"\"\n",
    "    Detect sentence types\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    sent_type: str\n",
    "        'state', 'ask', 'wish-excl' \n",
    "    \"\"\"\n",
    "    return sent_type in df\n",
    "\n",
    "sentence_type = classify_mood(train_data.text.tolist())\n",
    "train_data_feat = train_data.copy()\n",
    "train_data_feat[\"sentence_type\"] = sentence_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter sentences by \"type\"\n",
    "TYPE = 'ask'\n",
    "filtered = train_data_feat[train_data_feat.apply(lambda x: detect_sentence_type(x.sentence_type, TYPE), axis=1)]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_types = ['state', 'ask', 'wish-or-excl']\n",
    "data = train_data_feat\n",
    "\n",
    "button = widgets.Button(description=\"Click Me!\")\n",
    "display(button)\n",
    "\n",
    "@interact(WINDOW_START=(0, 100))\n",
    "def show_text_SideBySide(WINDOW_START):\n",
    "    \"\"\"\n",
    "    Show texts in dataframe side by side  \n",
    "    \n",
    "    INPUT:\n",
    "    -----\n",
    "    sent_types: list\n",
    "    data: pd.DataFrame\n",
    "\n",
    "    RETURN:    \n",
    "    ------\n",
    "    ipywidget object to display \n",
    "        display(show_text_SideBySide( ... ))\n",
    "    \"\"\"    \n",
    "    pd.set_option(\"display.max_colwidth\", -1)   #show entire text  \n",
    "    widget_all = []\n",
    "    count = -1\n",
    "    while count < len(sent_types)-1:        \n",
    "        count += 1        \n",
    "        widget_all.append(widgets.Output())    #init Outputs widgets\n",
    "        selected_df = data[['text']][  #select data by sent_type\n",
    "            data.apply(\n",
    "                lambda x: detect_sentence_type(\n",
    "                    x.sentence_type, sent_types[count]\n",
    "                    ), \n",
    "                    axis=1\n",
    "                )\n",
    "            ].iloc[WINDOW_START:WINDOW_START+10]\n",
    "        with widget_all[count]: \n",
    "            clear_output(True)\n",
    "            # print(\"Button clicked.\",round(random.uniform(0, 1),3))\n",
    "            display(selected_df)\n",
    "    return widgets.HBox(widget_all)\n",
    "\n",
    "button.on_click(show_text_SideBySide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# button = widgets.Button(description=\"Click Me!\")\n",
    "# out = widgets.Output()\n",
    "# display(button)\n",
    "# display(out)\n",
    "\n",
    "# def on_button_clicked(b):\n",
    "#     with out:\n",
    "#         clear_output(True)\n",
    "#         print(\"Button clicked.\",round(random.uniform(0, 1),3))\n",
    "\n",
    "# button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# References\n",
    "\n",
    "(1) https://allthingslinguistic.com/post/160783915317/being-a-declarative-or-interrogative-or  \n",
    "(2) https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780199283613.001.0001/acprof-9780199283613-chapter-6   \n",
    "(3) Fareh, S., & Moussa, M. B. (2008). Pragmatic Functions of Interrogative Sentences in English: A Corpus-based Study. International Journal of Arabic-English Studies, 9(1), 145-164.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('intent': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b321174aa8bda30359f2456b92364ad1fc4bb3682cd23b89ef7dfb1e69ab5093"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
